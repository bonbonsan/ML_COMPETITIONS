{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee66e39-6c91-4a18-97f0-65839c172766",
   "metadata": {},
   "source": [
    "# GPU speed comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b3f2e2d-a45b-418b-b557-59964db50cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b51e14-b0ac-40d5-ace1-268937d590a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Quadro RTX 4000\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())        # True expected\n",
    "print(torch.cuda.get_device_name(0))    # Quadro RTX 4000 expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9decc50d-1482-4e53-b21a-93e02d9107b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "cwd = os.getcwd()\n",
    "project_root = None\n",
    "    \n",
    "# Check if any of the specified directory names are in the current path\n",
    "for dir_name in (\"ML_competitions\", \"workspace\"):\n",
    "    if dir_name in cwd:\n",
    "        project_root = cwd[:cwd.index(dir_name) + len(dir_name)]\n",
    "        break\n",
    "    \n",
    "if project_root:\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.append(project_root)\n",
    "        print(f\"Added to sys.path: {project_root}\")\n",
    "else:\n",
    "    raise RuntimeError(f\"None of {root_dir_names} found in current path: {cwd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f018fd-5a19-4619-bbc7-60339e27c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from my_library.models.custom_tabn import CustomTabNetModel\n",
    "from my_library.configs.model_configs.tabn_configs import TabNetConfig\n",
    "from my_library.configs.model_configs.fit_configs import FitConfig\n",
    "from my_library.validations.validation_runner import ValidationRunner\n",
    "from my_library.parameter_tunings.optuna_tuner import OptunaValidationTuner\n",
    "from my_library.utils.timeit import timeit\n",
    "from my_library.splitters.cross_validation import CrossValidationSplitter\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb7d28eb-a83a-4010-85db-c0c2122786eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 擬似巨大データの生成\n",
    "X, y = make_classification(n_samples=100_000, n_features=200, n_informative=50, n_redundant=50, random_state=42)\n",
    "X = pd.DataFrame(X, columns=[f\"feat_{i}\" for i in range(X.shape[1])])\n",
    "y = pd.Series(y, name=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93dc34ef-722a-4060-97ac-7c5a5296ed59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 200), (100000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8294767-fe43-4081-9e30-62d9ddaabdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共通設定\n",
    "feats = X.columns.tolist()\n",
    "fit_config = FitConfig(feats=feats, early_stopping_rounds=10, epochs=100, batch_size=1024)\n",
    "folds = CrossValidationSplitter(n_splits=5, shuffle=True, random_state=42).split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8224f635-31ec-421c-8bb0-7495e57f9c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行関数の定義\n",
    "@timeit\n",
    "def run_runner(use_gpu: bool, parallel_mode: str):\n",
    "    config = TabNetConfig(\n",
    "        task_type=\"classification\",\n",
    "        params=dict(n_d=8, n_a=8, n_steps=3, batch_size=1024, max_epochs=100),\n",
    "        use_gpu=use_gpu\n",
    "    )\n",
    "    runner = ValidationRunner(\n",
    "        model_class=CustomTabNetModel,\n",
    "        model_configs=config,\n",
    "        metric_fn=roc_auc_score,\n",
    "        predict_proba=True,\n",
    "        parallel_mode=parallel_mode\n",
    "    )\n",
    "    results = runner.run(folds, fit_config)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c5d8e4e-3eaf-493c-ae09-889019799337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 13:33:23,945 - my_library.validations.validation_runner - INFO - Initialized ValidationRunner | model=CustomTabNetModel task=classification\n",
      "2025-05-08 13:33:23,946 - my_library.validations.validation_runner - INFO - Validation run started.\n",
      "2025-05-08 13:33:23,947 - CustomTabNetModel - INFO - Initialized CustomTabNetModel on device=cuda.\n",
      "2025-05-08 13:33:23,948 - CustomTabNetModel - INFO - Initialized CustomTabNetModel on device=cuda.\n",
      "2025-05-08 13:33:23,948 - my_library.validations.validation_runner - INFO - [Fold 1] Train=(80000, 200), Valid=(20000, 200),                 use_gpu=True\n",
      "2025-05-08 13:33:23,949 - my_library.validations.validator - INFO - Initialized Validator for model: CustomTabNetModel\n",
      "2025-05-08 13:33:23,949 - my_library.validations.validator - INFO - Training start | model=CustomTabNetModel samples=80000 features=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 20 and best_valid_accuracy = 0.9916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "2025-05-08 13:35:28,232 - CustomTabNetModel - INFO - TabNet training completed.\n",
      "2025-05-08 13:35:28,234 - my_library.validations.validator - INFO - Training completed.\n",
      "2025-05-08 13:35:28,568 - my_library.validations.validation_runner - INFO - [Fold 1] Score=0.9916\n",
      "2025-05-08 13:35:28,570 - CustomTabNetModel - INFO - Initialized CustomTabNetModel on device=cuda.\n",
      "2025-05-08 13:35:28,570 - my_library.validations.validation_runner - INFO - [Fold 2] Train=(80000, 200), Valid=(20000, 200),                 use_gpu=True\n",
      "2025-05-08 13:35:28,571 - my_library.validations.validator - INFO - Initialized Validator for model: CustomTabNetModel\n",
      "2025-05-08 13:35:28,571 - my_library.validations.validator - INFO - Training start | model=CustomTabNetModel samples=80000 features=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 13 and best_valid_accuracy = 0.99275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "2025-05-08 13:37:13,225 - CustomTabNetModel - INFO - TabNet training completed.\n",
      "2025-05-08 13:37:13,227 - my_library.validations.validator - INFO - Training completed.\n",
      "2025-05-08 13:37:13,560 - my_library.validations.validation_runner - INFO - [Fold 2] Score=0.9928\n",
      "2025-05-08 13:37:13,562 - CustomTabNetModel - INFO - Initialized CustomTabNetModel on device=cuda.\n",
      "2025-05-08 13:37:13,562 - my_library.validations.validation_runner - INFO - [Fold 3] Train=(80000, 200), Valid=(20000, 200),                 use_gpu=True\n",
      "2025-05-08 13:37:13,563 - my_library.validations.validator - INFO - Initialized Validator for model: CustomTabNetModel\n",
      "2025-05-08 13:37:13,563 - my_library.validations.validator - INFO - Training start | model=CustomTabNetModel samples=80000 features=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 68 with best_epoch = 48 and best_valid_accuracy = 0.99175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "2025-05-08 13:40:33,705 - CustomTabNetModel - INFO - TabNet training completed.\n",
      "2025-05-08 13:40:33,706 - my_library.validations.validator - INFO - Training completed.\n",
      "2025-05-08 13:40:34,032 - my_library.validations.validation_runner - INFO - [Fold 3] Score=0.9917\n",
      "2025-05-08 13:40:34,033 - CustomTabNetModel - INFO - Initialized CustomTabNetModel on device=cuda.\n",
      "2025-05-08 13:40:34,034 - my_library.validations.validation_runner - INFO - [Fold 4] Train=(80000, 200), Valid=(20000, 200),                 use_gpu=True\n",
      "2025-05-08 13:40:34,034 - my_library.validations.validator - INFO - Initialized Validator for model: CustomTabNetModel\n",
      "2025-05-08 13:40:34,035 - my_library.validations.validator - INFO - Training start | model=CustomTabNetModel samples=80000 features=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 21 and best_valid_accuracy = 0.9919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "2025-05-08 13:42:43,665 - CustomTabNetModel - INFO - TabNet training completed.\n",
      "2025-05-08 13:42:43,667 - my_library.validations.validator - INFO - Training completed.\n",
      "2025-05-08 13:42:44,001 - my_library.validations.validation_runner - INFO - [Fold 4] Score=0.9919\n",
      "2025-05-08 13:42:44,002 - CustomTabNetModel - INFO - Initialized CustomTabNetModel on device=cuda.\n",
      "2025-05-08 13:42:44,002 - my_library.validations.validation_runner - INFO - [Fold 5] Train=(80000, 200), Valid=(20000, 200),                 use_gpu=True\n",
      "2025-05-08 13:42:44,003 - my_library.validations.validator - INFO - Initialized Validator for model: CustomTabNetModel\n",
      "2025-05-08 13:42:44,003 - my_library.validations.validator - INFO - Training start | model=CustomTabNetModel samples=80000 features=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 49 with best_epoch = 29 and best_valid_accuracy = 0.9919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "2025-05-08 13:45:15,027 - CustomTabNetModel - INFO - TabNet training completed.\n",
      "2025-05-08 13:45:15,029 - my_library.validations.validator - INFO - Training completed.\n",
      "2025-05-08 13:45:15,359 - my_library.validations.validation_runner - INFO - [Fold 5] Score=0.9919\n",
      "2025-05-08 13:45:15,360 - my_library.validations.validation_runner - INFO - Validation completed | mean=0.9920 std=0.0004\n"
     ]
    }
   ],
   "source": [
    "# 時間計測（3パターン）\n",
    "# results_cpu_serial = run_runner(use_gpu=False, parallel_mode=False)\n",
    "# results_cpu_parallel = run_runner(use_gpu=False, parallel_mode=True)\n",
    "results_gpu_serial = run_runner(use_gpu=True, parallel_mode=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4789a92-3459-4c21-9458-775e272d83bf",
   "metadata": {},
   "source": [
    "1504.8384 sec, 2142.0529 sec, 710secくらい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04b6426f-e72c-4257-8e0f-84ac344622c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OptunaによるGPUチューニング\n",
    "@timeit\n",
    "def run_gpu_optuna():\n",
    "    config = TabNetConfig(\n",
    "        task_type=\"classification\",\n",
    "        params={},  # 初期は空でもOK\n",
    "        use_gpu=True\n",
    "    )\n",
    "    tuner = OptunaValidationTuner(\n",
    "        model_class=CustomTabNetModel,\n",
    "        model_configs=config,\n",
    "        param_space={\n",
    "            \"n_d\": [8, 16],\n",
    "            \"n_steps\": [3, 5],\n",
    "            \"learning_rate\": (0.01, 0.2),\n",
    "        },\n",
    "        folds=folds,\n",
    "        scoring=roc_auc_score,\n",
    "        predict_proba=True,\n",
    "        n_trials=5,\n",
    "        maximize=True,\n",
    "        parallel_mode=False,\n",
    "    )\n",
    "    return tuner.tune(fit_config, return_runner_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93348d2d-da86-4271-8616-0aa1f615636d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-08 13:46:21,675] A new study created in memory with name: no-name-a2ae1fda-e04b-4bab-9e2d-182727bb528b\n",
      "2025-05-08 13:46:21,678 - my_library.validations.validation_runner - INFO - Initialized ValidationRunner | model=CustomTabNetModel task=classification\n",
      "2025-05-08 13:46:21,679 - my_library.validations.validation_runner - INFO - Validation run started.\n",
      "2025-05-08 13:46:21,680 - CustomTabNetModel - INFO - Initialized CustomTabNetModel on device=cuda.\n",
      "2025-05-08 13:46:21,681 - CustomTabNetModel - INFO - Initialized CustomTabNetModel on device=cuda.\n",
      "2025-05-08 13:46:21,682 - my_library.validations.validation_runner - INFO - [Fold 1] Train=(80000, 200), Valid=(20000, 200),                 use_gpu=True\n",
      "2025-05-08 13:46:21,683 - my_library.validations.validator - INFO - Initialized Validator for model: CustomTabNetModel\n",
      "2025-05-08 13:46:21,683 - my_library.validations.validator - INFO - Training start | model=CustomTabNetModel samples=80000 features=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 58 with best_epoch = 38 and best_valid_accuracy = 0.99155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "2025-05-08 13:50:32,772 - CustomTabNetModel - INFO - TabNet training completed.\n",
      "2025-05-08 13:50:32,773 - my_library.validations.validator - INFO - Training completed.\n",
      "2025-05-08 13:50:33,227 - my_library.validations.validation_runner - INFO - [Fold 1] Score=0.9915\n",
      "2025-05-08 13:50:33,228 - CustomTabNetModel - INFO - Initialized CustomTabNetModel on device=cuda.\n",
      "2025-05-08 13:50:33,229 - my_library.validations.validation_runner - INFO - [Fold 2] Train=(80000, 200), Valid=(20000, 200),                 use_gpu=True\n",
      "2025-05-08 13:50:33,229 - my_library.validations.validator - INFO - Initialized Validator for model: CustomTabNetModel\n",
      "2025-05-08 13:50:33,230 - my_library.validations.validator - INFO - Training start | model=CustomTabNetModel samples=80000 features=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 47 with best_epoch = 27 and best_valid_accuracy = 0.99345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "2025-05-08 13:53:58,360 - CustomTabNetModel - INFO - TabNet training completed.\n",
      "2025-05-08 13:53:58,362 - my_library.validations.validator - INFO - Training completed.\n",
      "2025-05-08 13:53:58,806 - my_library.validations.validation_runner - INFO - [Fold 2] Score=0.9934\n",
      "2025-05-08 13:53:58,808 - CustomTabNetModel - INFO - Initialized CustomTabNetModel on device=cuda.\n",
      "2025-05-08 13:53:58,808 - my_library.validations.validation_runner - INFO - [Fold 3] Train=(80000, 200), Valid=(20000, 200),                 use_gpu=True\n",
      "2025-05-08 13:53:58,809 - my_library.validations.validator - INFO - Initialized Validator for model: CustomTabNetModel\n",
      "2025-05-08 13:53:58,809 - my_library.validations.validator - INFO - Training start | model=CustomTabNetModel samples=80000 features=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 43 with best_epoch = 23 and best_valid_accuracy = 0.99215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "2025-05-08 13:57:09,703 - CustomTabNetModel - INFO - TabNet training completed.\n",
      "2025-05-08 13:57:09,705 - my_library.validations.validator - INFO - Training completed.\n",
      "2025-05-08 13:57:10,195 - my_library.validations.validation_runner - INFO - [Fold 3] Score=0.9921\n",
      "2025-05-08 13:57:10,196 - CustomTabNetModel - INFO - Initialized CustomTabNetModel on device=cuda.\n",
      "2025-05-08 13:57:10,196 - my_library.validations.validation_runner - INFO - [Fold 4] Train=(80000, 200), Valid=(20000, 200),                 use_gpu=True\n",
      "2025-05-08 13:57:10,197 - my_library.validations.validator - INFO - Initialized Validator for model: CustomTabNetModel\n",
      "2025-05-08 13:57:10,198 - my_library.validations.validator - INFO - Training start | model=CustomTabNetModel samples=80000 features=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 18 and best_valid_accuracy = 0.99185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "2025-05-08 14:00:03,931 - CustomTabNetModel - INFO - TabNet training completed.\n",
      "2025-05-08 14:00:03,933 - my_library.validations.validator - INFO - Training completed.\n",
      "2025-05-08 14:00:04,385 - my_library.validations.validation_runner - INFO - [Fold 4] Score=0.9918\n",
      "2025-05-08 14:00:04,387 - CustomTabNetModel - INFO - Initialized CustomTabNetModel on device=cuda.\n",
      "2025-05-08 14:00:04,387 - my_library.validations.validation_runner - INFO - [Fold 5] Train=(80000, 200), Valid=(20000, 200),                 use_gpu=True\n",
      "2025-05-08 14:00:04,388 - my_library.validations.validator - INFO - Initialized Validator for model: CustomTabNetModel\n",
      "2025-05-08 14:00:04,388 - my_library.validations.validator - INFO - Training start | model=CustomTabNetModel samples=80000 features=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 64 with best_epoch = 44 and best_valid_accuracy = 0.9929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "2025-05-08 14:04:37,168 - CustomTabNetModel - INFO - TabNet training completed.\n",
      "2025-05-08 14:04:37,170 - my_library.validations.validator - INFO - Training completed.\n",
      "2025-05-08 14:04:37,647 - my_library.validations.validation_runner - INFO - [Fold 5] Score=0.9929\n",
      "2025-05-08 14:04:37,648 - my_library.validations.validation_runner - INFO - Validation completed | mean=0.9924 std=0.0007\n",
      "[I 2025-05-08 14:04:37,649] Trial 0 finished with value: 0.9923788120673445 and parameters: {'n_d': 16, 'n_steps': 5, 'learning_rate': 0.02413030200823877}. Best is trial 0 with value: 0.9923788120673445.\n",
      "2025-05-08 14:04:37,651 - my_library.validations.validation_runner - INFO - Initialized ValidationRunner | model=CustomTabNetModel task=classification\n",
      "2025-05-08 14:04:37,651 - my_library.validations.validation_runner - INFO - Validation run started.\n",
      "2025-05-08 14:04:37,652 - CustomTabNetModel - INFO - Initialized CustomTabNetModel on device=cuda.\n",
      "2025-05-08 14:04:37,652 - CustomTabNetModel - INFO - Initialized CustomTabNetModel on device=cuda.\n",
      "2025-05-08 14:04:37,654 - my_library.validations.validation_runner - INFO - [Fold 1] Train=(80000, 200), Valid=(20000, 200),                 use_gpu=True\n",
      "2025-05-08 14:04:37,654 - my_library.validations.validator - INFO - Initialized Validator for model: CustomTabNetModel\n",
      "2025-05-08 14:04:37,655 - my_library.validations.validator - INFO - Training start | model=CustomTabNetModel samples=80000 features=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 58 with best_epoch = 38 and best_valid_accuracy = 0.99155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "2025-05-08 14:08:47,902 - CustomTabNetModel - INFO - TabNet training completed.\n",
      "2025-05-08 14:08:47,903 - my_library.validations.validator - INFO - Training completed.\n",
      "2025-05-08 14:08:48,359 - my_library.validations.validation_runner - INFO - [Fold 1] Score=0.9915\n",
      "2025-05-08 14:08:48,361 - CustomTabNetModel - INFO - Initialized CustomTabNetModel on device=cuda.\n",
      "2025-05-08 14:08:48,361 - my_library.validations.validation_runner - INFO - [Fold 2] Train=(80000, 200), Valid=(20000, 200),                 use_gpu=True\n",
      "2025-05-08 14:08:48,362 - my_library.validations.validator - INFO - Initialized Validator for model: CustomTabNetModel\n",
      "2025-05-08 14:08:48,362 - my_library.validations.validator - INFO - Training start | model=CustomTabNetModel samples=80000 features=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 47 with best_epoch = 27 and best_valid_accuracy = 0.99345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "2025-05-08 14:12:16,852 - CustomTabNetModel - INFO - TabNet training completed.\n",
      "2025-05-08 14:12:16,853 - my_library.validations.validator - INFO - Training completed.\n",
      "2025-05-08 14:12:17,286 - my_library.validations.validation_runner - INFO - [Fold 2] Score=0.9934\n",
      "2025-05-08 14:12:17,287 - CustomTabNetModel - INFO - Initialized CustomTabNetModel on device=cuda.\n",
      "2025-05-08 14:12:17,288 - my_library.validations.validation_runner - INFO - [Fold 3] Train=(80000, 200), Valid=(20000, 200),                 use_gpu=True\n",
      "2025-05-08 14:12:17,289 - my_library.validations.validator - INFO - Initialized Validator for model: CustomTabNetModel\n",
      "2025-05-08 14:12:17,289 - my_library.validations.validator - INFO - Training start | model=CustomTabNetModel samples=80000 features=200\n",
      "[W 2025-05-08 14:13:28,254] Trial 1 failed with parameters: {'n_d': 16, 'n_steps': 5, 'learning_rate': 0.12212189215860647} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/workspace/my_library/parameter_tunings/optuna_tuner.py\", line 50, in _objective\n",
      "    result = runner.run(self.folds, self._fit_config)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/my_library/validations/validation_runner.py\", line 107, in run\n",
      "    self.results = self._run_folds_sequential(folds, fit_config)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/my_library/validations/validation_runner.py\", line 190, in _run_folds_sequential\n",
      "    score, pred, model, param = self._run_fold(i, X_tr, y_tr, X_val, y_val, fit_config)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/my_library/validations/validation_runner.py\", line 151, in _run_fold\n",
      "    validator.train(X_tr, y_tr, fit_config=fold_fc)\n",
      "  File \"/workspace/my_library/validations/validator.py\", line 59, in train\n",
      "    self.model.fit(X_train, y_train, fit_config=fit_config)\n",
      "  File \"/workspace/my_library/models/custom_tabn.py\", line 132, in fit\n",
      "    self.model.fit(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/abstract_model.py\", line 258, in fit\n",
      "    self._train_epoch(train_dataloader)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/abstract_model.py\", line 489, in _train_epoch\n",
      "    batch_logs = self._train_batch(X, y)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/abstract_model.py\", line 527, in _train_batch\n",
      "    output, M_loss = self.network(X)\n",
      "                     ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/tab_network.py\", line 616, in forward\n",
      "    return self.tabnet(x)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/tab_network.py\", line 492, in forward\n",
      "    steps_output, M_loss = self.encoder(x)\n",
      "                           ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/tab_network.py\", line 181, in forward\n",
      "    out = self.feat_transformers[step](masked_x)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/tab_network.py\", line 737, in forward\n",
      "    x = self.shared(x)\n",
      "        ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/tab_network.py\", line 774, in forward\n",
      "    x = self.glu_layers[0](x)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/tab_network.py\", line 804, in forward\n",
      "    x = self.bn(x)\n",
      "        ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/tab_network.py\", line 35, in forward\n",
      "    chunks = x.chunk(int(np.ceil(x.shape[0] / self.virtual_batch_size)), 0)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-05-08 14:13:28,258] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m best_params, best_score, best_result = \u001b[43mrun_gpu_optuna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mrun_gpu_optuna\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      4\u001b[39m config = TabNetConfig(\n\u001b[32m      5\u001b[39m     task_type=\u001b[33m\"\u001b[39m\u001b[33mclassification\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     params={},  \u001b[38;5;66;03m# 初期は空でもOK\u001b[39;00m\n\u001b[32m      7\u001b[39m     use_gpu=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      8\u001b[39m )\n\u001b[32m      9\u001b[39m tuner = OptunaValidationTuner(\n\u001b[32m     10\u001b[39m     model_class=CustomTabNetModel,\n\u001b[32m     11\u001b[39m     model_configs=config,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     parallel_mode=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     23\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfit_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_runner_results\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/my_library/parameter_tunings/optuna_tuner.py:92\u001b[39m, in \u001b[36mOptunaValidationTuner.tune\u001b[39m\u001b[34m(self, fit_config, return_runner_results)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Create study and optimize\u001b[39;00m\n\u001b[32m     89\u001b[39m study = optuna.create_study(\n\u001b[32m     90\u001b[39m     direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maximize \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     91\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# best_score already tracked as raw mean_score; update best_params\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.best_params = study.best_params\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/my_library/parameter_tunings/optuna_tuner.py:50\u001b[39m, in \u001b[36mOptunaValidationTuner._objective\u001b[39m\u001b[34m(self, trial)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Run validation\u001b[39;00m\n\u001b[32m     43\u001b[39m runner = ValidationRunner(\n\u001b[32m     44\u001b[39m     model_class=\u001b[38;5;28mself\u001b[39m.model_class,\n\u001b[32m     45\u001b[39m     model_configs=\u001b[38;5;28mself\u001b[39m.model_configs,\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m     parallel_mode=\u001b[38;5;28mself\u001b[39m.parallel_mode\n\u001b[32m     49\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m result = \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m score = result[\u001b[33m\"\u001b[39m\u001b[33mmean_score\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Track best raw score\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/my_library/validations/validation_runner.py:107\u001b[39m, in \u001b[36mValidationRunner.run\u001b[39m\u001b[34m(self, folds, fit_config)\u001b[39m\n\u001b[32m    104\u001b[39m use_gpu = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model_class(\u001b[38;5;28mself\u001b[39m.model_configs), \u001b[33m\"\u001b[39m\u001b[33muse_gpu\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_gpu \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parallel_mode:\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28mself\u001b[39m.results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_folds_sequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    109\u001b[39m     \u001b[38;5;28mself\u001b[39m.results = \u001b[38;5;28mself\u001b[39m._run_folds_parallel(folds, fit_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/my_library/validations/validation_runner.py:190\u001b[39m, in \u001b[36mValidationRunner._run_folds_sequential\u001b[39m\u001b[34m(self, folds, fit_config)\u001b[39m\n\u001b[32m    188\u001b[39m scores, preds, models, params = [], [], [], []\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, ((X_tr, y_tr), (X_val, y_val)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(folds):\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     score, pred, model, param = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m     scores.append(score)\n\u001b[32m    192\u001b[39m     preds.append(pred)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/my_library/validations/validation_runner.py:151\u001b[39m, in \u001b[36mValidationRunner._run_fold\u001b[39m\u001b[34m(self, i, X_tr, y_tr, X_val, y_val, fit_config)\u001b[39m\n\u001b[32m    143\u001b[39m fold_fc = FitConfig(\n\u001b[32m    144\u001b[39m     feats=fit_config.feats,\n\u001b[32m    145\u001b[39m     eval_set=[(X_val, y_val)],\n\u001b[32m   (...)\u001b[39m\u001b[32m    148\u001b[39m     batch_size=fit_config.batch_size\n\u001b[32m    149\u001b[39m )\n\u001b[32m    150\u001b[39m validator = Validator(model)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfold_fc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predict_proba \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_configs.task_type == \u001b[33m\"\u001b[39m\u001b[33mclassification\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    154\u001b[39m     proba = validator.predict_proba(X_val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/my_library/validations/validator.py:59\u001b[39m, in \u001b[36mValidator.train\u001b[39m\u001b[34m(self, X_train, y_train, fit_config)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# call custom fit with unified interface\u001b[39;00m\n\u001b[32m     55\u001b[39m logger.info(\n\u001b[32m     56\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining start | model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msamples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     58\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mTraining completed.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/my_library/models/custom_tabn.py:132\u001b[39m, in \u001b[36mCustomTabNetModel.fit\u001b[39m\u001b[34m(self, X, y, fit_config)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# build and train\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;28mself\u001b[39m.build_model()\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_np\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_np\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43meval_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28mself\u001b[39m.logger.info(\u001b[33m\"\u001b[39m\u001b[33mTabNet training completed.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/abstract_model.py:258\u001b[39m, in \u001b[36mTabModel.fit\u001b[39m\u001b[34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.max_epochs):\n\u001b[32m    254\u001b[39m \n\u001b[32m    255\u001b[39m     \u001b[38;5;66;03m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;28mself\u001b[39m._callback_container.on_epoch_begin(epoch_idx)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m     \u001b[38;5;66;03m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m eval_name, valid_dataloader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(eval_names, valid_dataloaders):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/abstract_model.py:489\u001b[39m, in \u001b[36mTabModel._train_epoch\u001b[39m\u001b[34m(self, train_loader)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[32m    487\u001b[39m     \u001b[38;5;28mself\u001b[39m._callback_container.on_batch_begin(batch_idx)\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     batch_logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m     \u001b[38;5;28mself\u001b[39m._callback_container.on_batch_end(batch_idx, batch_logs)\n\u001b[32m    493\u001b[39m epoch_logs = {\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._optimizer.param_groups[-\u001b[32m1\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/abstract_model.py:527\u001b[39m, in \u001b[36mTabModel._train_batch\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.network.parameters():\n\u001b[32m    525\u001b[39m     param.grad = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m output, M_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.compute_loss(output, y)\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Add the overall sparsity loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/tab_network.py:616\u001b[39m, in \u001b[36mTabNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    615\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.embedder(x)\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtabnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/tab_network.py:492\u001b[39m, in \u001b[36mTabNetNoEmbeddings.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    491\u001b[39m     res = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     steps_output, M_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     res = torch.sum(torch.stack(steps_output, dim=\u001b[32m0\u001b[39m), dim=\u001b[32m0\u001b[39m)\n\u001b[32m    495\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_multi_task:\n\u001b[32m    496\u001b[39m         \u001b[38;5;66;03m# Result will be in list format\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/tab_network.py:181\u001b[39m, in \u001b[36mTabNetEncoder.forward\u001b[39m\u001b[34m(self, x, prior)\u001b[39m\n\u001b[32m    179\u001b[39m M_feature_level = torch.matmul(M, \u001b[38;5;28mself\u001b[39m.group_attention_matrix)\n\u001b[32m    180\u001b[39m masked_x = torch.mul(M_feature_level, x)\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeat_transformers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m d = ReLU()(out[:, : \u001b[38;5;28mself\u001b[39m.n_d])\n\u001b[32m    183\u001b[39m steps_output.append(d)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/tab_network.py:737\u001b[39m, in \u001b[36mFeatTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshared\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    738\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.specifics(x)\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/tab_network.py:774\u001b[39m, in \u001b[36mGLU_Block.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    772\u001b[39m scale = torch.sqrt(torch.FloatTensor([\u001b[32m0.5\u001b[39m]).to(x.device))\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.first:  \u001b[38;5;66;03m# the first layer of the block has no scale multiplication\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mglu_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    775\u001b[39m     layers_left = \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.n_glu)\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/tab_network.py:804\u001b[39m, in \u001b[36mGLU_Layer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    803\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.fc(x)\n\u001b[32m--> \u001b[39m\u001b[32m804\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    805\u001b[39m     out = torch.mul(x[:, : \u001b[38;5;28mself\u001b[39m.output_dim], torch.sigmoid(x[:, \u001b[38;5;28mself\u001b[39m.output_dim :]))\n\u001b[32m    806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/tab_network.py:35\u001b[39m, in \u001b[36mGBN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     chunks = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mceil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     res = [\u001b[38;5;28mself\u001b[39m.bn(x_) \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(res, dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "best_params, best_score, best_result = run_gpu_optuna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c940a31-7494-4e7d-b3c5-e07702136575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
